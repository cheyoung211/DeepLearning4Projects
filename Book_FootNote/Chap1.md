# 1장 각주 모음
## 19 page
  19. [바운딩 박스](https://wiki.openstreetmap.org/wiki/Bounding_Box)
  20. [Free Space Detection with Deep Nets for Autonomous Driving](http://cs231n.stanford.edu/reports/2015/pdfs/jpazhaya_final.pdf).
  
## 20 page
  21. FCN: 완전한 콘볼루션 네트워크  [논문 참조: ⟨Fully Convolutional Networks for Semantic Segmentation, Jonathan Long, 2015⟩](https://arxiv.org/pdf/1411.4038.pdf)

  22. 의학 영상에서 장기의 병변을 구별 및 분리하는 이미지 분할 [그림참조](https://blogs.nvidia.co.kr/2021/12/17/nvidia-data-scientists-take-top-spots-in-miccai-2021-brain-tumor-segmentation-challenge/)
  23. 인공지능, 머신러닝, 딥러닝의 역사 [그림참조](https://developer.nvidia.com/deep-learning)

## 21 page
  24. [Artificial_intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence)
  25. [Lighthill_report](https://en.wikipedia.org/wiki/Lighthill_report)
  26. [Bayesian_probability](https://en.wikipedia.org/wiki/Bayesian_probability)
  27. [Fuzzy_logic](https://en.wikipedia.org/wiki/Fuzzy_logic)

## 22 page
  28. [Deep_learning](https://en.wikipedia.org/wiki/Deep_learning)
  29. [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](http://www.image-net.org/challenges/LSVRC/)
  30. 오류율은 영어로 error rate입니다. error는 loss와 같은 용어이며, 딥러닝 모델의 정확도와 대비하는 경우 대부분 loss를 사용하지만, 비율의 뜻으로 표현할 때는 error rate라는 용어를 많이 사용합니다.
  31. [cs231n Lecture 9 | CNN Architectures](https://youtu.be/DAOcjicFr1Y)

## 23 page
  32. 엔비디아 자율주행 자동차를 위한 딥러닝 영상분석 [그림 참조](https://youtu.be/1W9q5SjaJTc)
  33. 지도 학습: Hands-On Machine Learning with Scikit-Learn & Tensorflow.


## 24 page
  34. k-Nearest Neighbor: https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm
      SVM(supported Vector Machine): https://en.wikipedia.org/wiki/Support-vector_machine
      결정 나무(Decision Tree): [결정_트리_학습법](https://ko.wikipedia.org/wiki/결정_트리_학습법)

  35. [cs231n Lecture 2 | Image Classification](https://youtu.be/OoUX-nOEjG0)
  36. [Linear classification](https://en.wikipedia.org/wiki/Linear_classifier)
  37. 여기서 가중치는 행렬값이며 행렬은 파라미터들로 이뤄져 있습니다.
  
## 27 page
  38. 훈련 데이터: 빅데이터라고 부를 수 있는 많은 데이터세트
  39. [Inference](https://en.wikipedia.org/wiki/Inference).

## 28 page
  40. [Frame_rate](https://en.wikipedia.org/wiki/Frame_rate)

## 29 page
  41. 최적화(optimization) [cs231n Lecture 3 | Loss Functions and Optimization](https://youtu.be/h7iBpEHGVNc)

## 32 page
  42. 교차 엔트로피 오차(Cross Entropy Error) : ≪밑바닥부터 시작하는 딥러닝≫(한빛미디어, 2017), 114쪽.

## 34 page
  43. 경사 하강법(Gradient Descent): [cs231n Lecture 3 | Loss Functions and Optimization](https://youtu.be/h7iBpEHGVNc)

## 35 page
  44. 가중치의 변화에 따른 손실의 감소 그래프: 그래프는 2차원만 표현하지만, 실제로는 3차원 계곡의 형태입니다.
  45. 손실이 감소하는 방향: 그림에서 빨간색 화살표 방향
  46. 극솟값(Local Minimum): [극값](https://ko.wikipedia.org/wiki/극값)

## 36 page
  47. 학습률(Learning Rate): ≪밑바닥부터 시작하는 딥러닝≫(한빛미디어, 2017), 131쪽
  48. 확률 경사 하강법(Stochastic Gradient Descent: SGD): ≪알고리즘으로 배우는 인공지능, 머신러닝, 딥러닝 입문≫(위키북스, 2016), 206쪽
  49. 확률적(Stochastic)으로 선택한 샘플 데이터: 미니 배치라고 표현합니다.

## 37 page
  50. 역전파 훈련 알고리즘: [⟨Learning Internal Representations by Error Propagation⟩, David E. Rumelhart, James L. McClelland, 1987](https://ieeexplore.ieee.org/document/6302929)
  51. 역전파 : 《핸즈온 머신러닝》(한빛미디어, 2018), 337쪽

## 38 page
  52. 역전파: 오차 역전파라고 호칭하기도 합니다. ⟪패턴인식과 머신러닝⟫(제이펍, 2018), 275쪽
  53. 그래디언트: 곡선에서 접선의 기울기를 말하며, 이는 곡선의 함숫값의 변화율을 나타냅니다. 그래디언트, 그레이디언트 등으로 표기하며, 이 책에서는 ‘그래디언트’로 표기합니다.
  54. 미분의 연쇄법칙, 체인룰: ≪밑바닥부터 시작하는 딥러닝≫(한빛미디어, 2017), 153쪽

## 40 page
  55. [역전파(Back Propagation)에 대해 직관적으로 설명한 참고 영상](https://www.youtube.com/watch?v=q555kfIFUCM)
  56. [하이퍼파라미터](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning))

## 41 page
  57. 소프트맥스 함수: ≪밑바닥부터 시작하는 딥러닝≫(한빛미디어, 2017), 92쪽
  58. 소프트맥스 함수는 딥러닝의 계층에서 출력층의 활성화 함수로 가장 적합: ≪핸즈온 머신러닝≫(한빛미디어, 2018), 217쪽

## 42 page
  59. MNIST 데이터세트: [https://en.wikipedia.org/wiki/MNIST_database)
  60. 심층 신경망의 방식을 인공신경망(Artificial Neural Network)이라고도 합니다.

