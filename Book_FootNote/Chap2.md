# 2장 각주 모음
## 46 page
  1. 콘볼루션 네트워크: ≪Deep Learning≫(MIT Press, 2016), Ian Goodfellow, etc, 360쪽

## 47 page
  2. 캐스케이드 분류기(Cascade Classifier): [Haar Feature-based Cascade Classifier for Object Detection⟩](https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html)
  3. [opencv](https://opencv.org/)
  4. [Data_mining](https://en.wikipedia.org/wiki/Data_mining)
  5. [K-nearest_neighbors_algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)

## 48 page
  6. FC(Fully Connected) 레이어: FC는 모든 노드가 완전히 연결되어 계산되기 때문에 ‘완전 연결’이라고도 합니다.
  7. : ≪Raspberry Pi Computer Vision≫(Oreilly, 2016), Simon Monk, 29쪽

## 49 page
  8. Conv 레이어: 콘볼루션 레이어는 콘볼루션 연산으로 이미지의 특징을 효과적으로 추출하는 레이어로, 뒤에 설명하는 풀링P(ooling) 레이어와 구분해 Conv 레이어라는 표현을 많이 사용합니다.
  9. 허블과 위젤이 고양이와 원숭이의 시각 세포 연구 [Hubel and Wiesel & the Neural Basis of Visual Perception](https://knowingneurons.com/2014/10/29/hubel-and-wiesel-the-neural-basisof-
visual-perception/)
  10. 콘볼루션 신경망: ≪알고리즘으로 배우는 인공지능, 머신러닝, 딥러닝 입문≫(위키북스, 2016), 270쪽

## 50 page
  11. [Convolution](https://homepages.inf.ed.ac.uk/rbf/HIPR2/convolve.htm)

## 51 page
  12. 콘볼루션 과정: [cs231n Lecture 5 | Convolutional Neural Networks](https://youtu.be/bNb2fEVKeEo)

## 52 page
  13. "Conv 레이어를 깊게 할수록": 레이어의 개수가 많아진다는 뜻입니다.
  14. [Visualizing and Understanding Convolutional Networks](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf), Matthew D. Zeiler and Rob Fergus, Matthew D. Zeiler and Rob Fergus, 2014
  15. 특징 맵: [cs231n Lecture 5 | Convolutional Neural Networks](https://youtu.be/bNb2fEVKeEo)

## 53 page
  16. CNN의 특성: ≪밑바닥부터 시작하는 딥러닝≫(한빛미디어, 2017), 268쪽
  17. 패딩(padding): [What is Padding in Machine Learning?](https://deepai.org/machine-learning-glossary-and-terms/padding)

## 54 page
  18. 결괏값인 스코어 벡터의 사이즈는 분류 클래스의 개수에 따라 다릅니다.
  19. [소프트맥스 함수(Softmax function)](https://en.wikipedia.org/wiki/Softmax_function) 

## 56 page
  20. [주성분 분석(PCA: Principal Component Analysis)](https://en.wikipedia.org/wiki/Principal_component_analysis)
  21. [whitening 변환](https://en.wikipedia.org/wiki/Whitening_transformation)
  22. [평균값 빼기(Subtract Mean)](https://stackoverflow.com/questions/44788133/how-does-mean-image-subtraction-work)

## 57 page
  23. 오버피팅의 예 [그림 참조](https://en.wikipedia.org/wiki/Overfitting)
  24. 평가 데이터: 밸리데이션(Validation) 데이터라고도 합니다.
## 58 page
  25. 딥러닝에서 평가 데이터의 역할 그림 참조 : 이러한 그래프는 [Digits](https://developer.nvidia.com/digits)라는 딥러닝 서버용 소프트웨어 프레임워크에서 얻을 수 있습니다.

## 59 page
  26. 평가 손실: ‘밸리데이션 손실’, ‘밸리데이션 에러’라고도 합니다.
  27. 정규화(Weight Regularization): ≪Deep Learning≫(MIT Press, 2016), 427쪽
  28. 오버피팅을 억제: ≪밑바닥부터 시작하는 딥러닝≫(한빛미디어, 2017), 217쪽
  29. [L1, L2 정규화](https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261)
  30. [드롭아웃(dropout)](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout)
  31. (데이터 증강(Data Augmentation)](https://www.tensorflow.org/tutorials/images/data_augmentation)





